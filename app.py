import streamlit as st
import logging
from typing import TypedDict, Optional, List, Tuple
from models.adaptive_rag import *
import streamlit.components.v1 as components
from models.adaptive_rag.main_arag import adaptive_rag_graph as compiled_app, GraphState
from models.manual_query.manual_search import search

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Configure Streamlit page
st.set_page_config(
    page_title="Chatbot AI - B·ªô T√†i Ch√≠nh",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Sidebar for additional information
with st.sidebar:
    st.header("‚ÑπÔ∏è Th√¥ng tin")
    st.markdown("""
    - **Chatbot AI** h·ªó tr·ª£ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn B·ªô T√†i Ch√≠nh Vi·ªát Nam ho·∫∑c t√¨m ki·∫øm th√¥ng tin chung.
    - D·ªØ li·ªáu ƒë∆∞·ª£c l·∫•y t·ª´ c∆° s·ªü tri th·ª©c n·ªôi b·ªô ho·∫∑c t√¨m ki·∫øm web.
    - ƒê·ªÉ c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t, h√£y ƒë·∫∑t c√¢u h·ªèi r√µ r√†ng v√† c·ª• th·ªÉ.
    """)
    st.markdown("---")
    option = st.radio(
        "L·ª±a ch·ªçn ch·ª©c nƒÉng:",
        ["H·ªèi ƒë√°p chung", "H·ªèi ƒë√°p theo chuy√™n ng√†nh", "T√¨m ki·∫øm vƒÉn b·∫£n ch√≠nh x√°c"],
        index=0
    )
    if option == "H·ªèi ƒë√°p theo chuy√™n ng√†nh":
        chuyen_nganh = st.selectbox(
            "Ch·ªçn chuy√™n ng√†nh:",
            ["B·∫£o hi·ªÉm", "B·∫•t ƒë·ªông s·∫£n", "B·ªô m√°y h√†nh ch√≠nh", 
             "Ch·ª©ng kho√°n", "C√¥ng ngh·ªá th√¥ng tin", "ƒê·∫ßu t∆∞", "D·ªãch v·ª• ph√°p l√Ω"]
        )
    st.markdown("---")
    st.caption("Powered by Team 4 - Ph·∫°m VƒÉn Thanh, Nguy·ªÖn Nam, Phan Thanh Th√°i, Ph·∫°m C√¥ng Chi·∫øn")

# Attempt to import the compiled app and state from the backend
try:
    if option != "T√¨m ki·∫øm vƒÉn b·∫£n ch√≠nh x√°c":
        compiled_app = adaptive_rag_graph
    else:
        compiled_app = search
    BACKEND_AVAILABLE = compiled_app is not None
except ImportError as e:
    logger.error(f"Failed to import backend 'qabot': {e}", exc_info=True)
    st.error("L·ªói: Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn h·ªá th·ªëng x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i sau.")
    compiled_app = None
    BACKEND_AVAILABLE = False
except Exception as e:
    logger.error(f"General error during backend initialization: {e}", exc_info=True)
    st.error("L·ªói: H·ªá th·ªëng x·ª≠ l√Ω g·∫∑p s·ª± c·ªë khi kh·ªüi t·∫°o. Vui l√≤ng th·ª≠ l·∫°i sau.")
    compiled_app = None
    BACKEND_AVAILABLE = False


# Main UI
st.title("ü§ñ Chatbot AI - B·ªô T√†i Ch√≠nh")
st.caption("Tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ gi·∫£i ƒë√°p c√°c th·∫Øc m·∫Øc v·ªÅ quy ƒë·ªãnh t√†i ch√≠nh ho·∫∑c th√¥ng tin chung.")
if option == "T√¨m ki·∫øm vƒÉn b·∫£n ch√≠nh x√°c":
    st.caption("B·∫°n ƒëang ch·ªçn ch·∫ø ƒë·ªô t√¨m ki·∫øm ch√≠nh x√°c t·ª´ vƒÉn b·∫£n. C√°c t√≠nh nƒÉng kh·∫£ d·ª•ng: T√¨m ki·∫øm n·ªôi dung vƒÉn b·∫£n, ng∆∞·ªùi k√Ω, ng√†y ban h√†nh, c√°c vƒÉn b·∫£n li√™n quan,...")

# Initialize chat history
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ch√†o b·∫°n! T√¥i c√≥ th·ªÉ gi√∫p g√¨ v·ªÅ c√°c quy ƒë·ªãnh t√†i ch√≠nh ho·∫∑c th√¥ng tin kh√°c?"}
    ]

# Display chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input and processing
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (v√≠ d·ª•: 'Quy ƒë·ªãnh v·ªÅ thu·∫ø TNCN 2023 l√† g√¨?')"):
    # Add user message to history and display
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Process if backend is available
    if not BACKEND_AVAILABLE:
        response_content = "Xin l·ªói, h·ªá th·ªëng x·ª≠ l√Ω hi·ªán kh√¥ng kh·∫£ d·ª•ng. Vui l√≤ng th·ª≠ l·∫°i sau."
        with st.chat_message("assistant"):
            st.error(response_content)
        st.session_state.messages.append({"role": "assistant", "content": response_content})
    else:
        # Display processing indicator
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("‚öôÔ∏è ƒêang x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n...")
            result = ""
            try:
                # Chu·∫©n b·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán t·ª´ st.session_state.messages
                history = []
                for i in range(0, len(st.session_state.messages) - 1):  # B·ªè qua tin nh·∫Øn hi·ªán t·∫°i (ƒëang x·ª≠ l√Ω)
                    msg = st.session_state.messages[i]
                    if msg["role"] == "user" and i + 1 < len(st.session_state.messages):
                        next_msg = st.session_state.messages[i + 1]
                        if next_msg["role"] == "assistant":
                            history.append((msg["content"], next_msg["content"]))

                # Initialize state with query and history
                initial_state = GraphState(question=prompt.strip(), generation=None,retry_count=0, history=history[-3:])  # Ch·ªâ l·∫•y 3 l∆∞·ª£t g·∫ßn nh·∫•t ƒë·ªÉ tr√°nh v∆∞·ª£t gi·ªõi h·∫°n
                logger.info(f"Processing query: {prompt} with history: {history}")

                # Invoke backend
                final_state = compiled_app.invoke(initial_state)

                # Process response
                result = final_state.get("generation","Kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi h·ª£p l·ªá.") 
                docs = final_state["documents"]
                print(type(docs))
                sources = "Internet"
                html_srcs = []
                if isinstance(docs, list):
                    sources = docs[0].metadata['source']
                    print(sources)
                    if not isinstance(sources, list):
                        sources = [sources]
                    for src in sources: 
                        doc_name = src.split('\\')[-1]
                        print(f"doc_name: {doc_name} ")
                        html_name = doc_name.replace('.txt', '.html')
                        html_src = "crawl/data/tvpl_new/html/" + html_name
                        print(html_src)
                        html_srcs.append(html_src)
                print(html_srcs)
                if not final_state.get("generation"):
                    logger.warning(f"No valid result for query: {prompt}")
                    message_placeholder.warning(result)
                else:
                    logger.info(f"Response generated: {result}...")
                    message_placeholder.markdown(result)
                    if len(html_srcs) > 0 : 
                        all_html = ""
                        for src in html_srcs:
                            logger.info(f"source: {src}")
                            with open(src, "r", encoding="utf-8") as f: 
                                all_html += f.read()
                                all_html += "<hr>"  # Th√™m ƒë∆∞·ªùng k·∫ª gi·ªØa c√°c file (t√πy ch·ªçn)
                        centered_html = f"""
                        <div style="display: flex; justify-content: center;">
                            <div style="width: 600px;">
                                {all_html}
                            </div>
                        </div>
                        """
                        components.html(centered_html,height=1500, width = 900 ,scrolling=True)
                    

            except Exception as e:
                logger.error(f"Error during query processing: {e}", exc_info=True)
                response_content = f"‚ùå ƒê√£ x·∫£y ra l·ªói: {e}. Vui l√≤ng th·ª≠ l·∫°i."
                message_placeholder.error(response_content)

            # Add assistant response to history
            st.session_state.messages.append({"role": "assistant", "content": result})