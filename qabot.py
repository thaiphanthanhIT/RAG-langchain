import logging
import os
from typing import TypedDict, Literal, Dict, Any

import google.generativeai as genai
from dotenv import load_dotenv
from langgraph.graph import StateGraph
from langchain_core.runnables import RunnableLambda
from langchain_community.embeddings import GPT4AllEmbeddings
from langchain_community.vectorstores import FAISS
from duckduckgo_search import DDGS
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# ---------------------------
# Logging
# ---------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


# Load environment
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")

if not api_key:
    raise ValueError("API key GOOGLE_API_KEY ch∆∞a ƒë∆∞·ª£c thi·∫øt l·∫≠p trong m√¥i tr∆∞·ªùng ho·∫∑c file .env.")

genai.configure(api_key=api_key)

# Kh·ªüi t·∫°o models
try:
    routing_model = genai.GenerativeModel("models/gemini-1.5-flash") 
    answering_model = genai.GenerativeModel("models/gemini-1.5-flash") 
    logger.info("ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng model Gemini.")
except Exception as e:
    logger.error(f"L·ªói kh·ªüi t·∫°o model Gemini: {e}")
    raise

# ƒê·ªãnh nghƒ©a tr·∫°ng th√°i LangGraph
class QAState(TypedDict):
    query: str
    # source: Literal["ministry", "search"] # C√≥ th·ªÉ b·ªè source n·∫øu route quy·∫øt ƒë·ªãnh tr·ª±c ti·∫øp node ch·∫°y
    result: str | None # Cho ph√©p None ƒë·ªÉ x·ª≠ l√Ω l·ªói


# Tool: Truy v·∫•n d·ªØ li·ªáu B·ªô T√†i ch√≠nh (FAISS)
def search_ministry(state: QAState) -> Dict[str, Any]:
    logger.info(">>> Running Tool: search_ministry (FAISS)")
    query = state.get("query")
    if not query:
         logger.error("search_ministry: No query found in state.")
         return {"result": "L·ªói: Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi trong state."}

    try:
        # Gi·∫£ s·ª≠ embedding model v√† DB ƒë√£ t·ªìn t·∫°i v√† ƒë√∫ng ƒë∆∞·ªùng d·∫´n
        embedding_model = GPT4AllEmbeddings(model_file="./all-MiniLM-L6-v2-f16.gguf")
        db = FAISS.load_local("vectorstores/db_text", embedding_model, allow_dangerous_deserialization=True)
        logger.info(f"Searching FAISS for: {query}")
        docs = db.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in docs])

        if not context.strip():
             logger.warning("search_ministry: No relevant documents found in FAISS.")
             return {"result": "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan trong c∆° s·ªü d·ªØ li·ªáu B·ªô T√†i ch√≠nh."}

        prompt = f"""S·ª≠ d·ª•ng CH·ªà th√¥ng tin trong ng·ªØ c·∫£nh sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. Kh√¥ng d√πng ki·∫øn th·ª©c b√™n ngo√†i.
Ng·ªØ c·∫£nh:
---
{context}
---
C√¢u h·ªèi: {query}

Tr·∫£ l·ªùi:"""
        logger.info(f"[search_ministry] Prompting Gemini...")
        # logger.debug(f"[search_ministry] Full Prompt:\n{prompt}") # Uncomment if needed

        response = answering_model.generate_content(prompt)

        # *** LOGGING CHI TI·∫æT PH·∫¢N H·ªíI GEMINI ***
        logger.info(f"[search_ministry] Raw Gemini Response Text: '{response.text}'")
        try:
            logger.info(f"[search_ministry] Gemini Prompt Feedback: {response.prompt_feedback}")
            # logger.info(f"[search_ministry] Gemini Response Parts: {response.parts}") # Uncomment if needed
        except Exception as log_e:
            logger.warning(f"[search_ministry] Cannot log detailed response info: {log_e}")
        # ******************************************

        final_result = response.text.strip()
        if not final_result:
             logger.warning("[search_ministry] Gemini returned empty or whitespace response.")
             return {"result": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ d·ªØ li·ªáu B·ªô T√†i ch√≠nh t√¨m ƒë∆∞·ª£c."}

        return {"result": final_result}

    except FileNotFoundError:
         logger.error("search_ministry: L·ªói kh√¥ng t√¨m th·∫•y file vectorstores/db_faiss ho·∫∑c model embedding.")
         return {"result": "L·ªói: Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu B·ªô T√†i ch√≠nh. Vui l√≤ng ki·ªÉm tra c√†i ƒë·∫∑t."}
    except Exception as e:
        logger.error(f"L·ªói trong search_ministry: {e}")
        return {"result": "ƒê√£ x·∫£y ra l·ªói khi truy v·∫•n d·ªØ li·ªáu B·ªô T√†i ch√≠nh."}

# Tool: T√¨m ki·∫øm DuckDuckGo
def search_web(state: QAState) -> Dict[str, Any]:
    logger.info(">>> Running Tool: search_web (DuckDuckGo)")
    query = state.get("query")
    if not query:
         logger.error("search_web: No query found in state.")
         return {"result": "L·ªói: Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi trong state."}

    try:
        logger.info(f"Searching Web (DDGS) for: {query}")
        with DDGS() as ddgs:
            # Gi·∫£m s·ªë l∆∞·ª£ng k·∫øt qu·∫£ ƒë·ªÉ prompt ng·∫Øn g·ªçn h∆°n, tr√°nh v∆∞·ª£t gi·ªõi h·∫°n
            results = list(ddgs.text(query, max_results=3)) # Gi·∫£m xu·ªëng 3 k·∫øt qu·∫£

        if not results:
            logger.warning("search_web: No results found on DuckDuckGo.")
            return {"result": "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan tr√™n web."}

        context = "\n".join([
            f"Ti√™u ƒë·ªÅ: {r.get('title', '')}\nN·ªôi dung: {r.get('body', '')}\nNgu·ªìn: {r.get('href', '')}\n---"
            for r in results if r.get('body') # Ch·ªâ l·∫•y k·∫øt qu·∫£ c√≥ n·ªôi dung
        ]).strip()

        if not context:
            logger.warning("search_web: No content found in DDGS results.")
            return {"result": "Kh√¥ng t√¨m th·∫•y n·ªôi dung c·ª• th·ªÉ t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm web."}

        # Prompt y√™u c·∫ßu LLM tr·∫£ l·ªùi D·ª∞A TR√äN k·∫øt qu·∫£ t√¨m ki·∫øm
        prompt = f"""D·ª±a CH·ª¶ Y·∫æU v√†o c√°c k·∫øt qu·∫£ t√¨m ki·∫øm sau ƒë√¢y ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ng·∫Øn g·ªçn v√† ch√≠nh x√°c. Tr√≠ch d·∫´n ngu·ªìn n·∫øu c√≥ th·ªÉ.
K·∫øt qu·∫£ t√¨m ki·∫øm:
---
{context}
---
C√¢u h·ªèi: {query}

Tr·∫£ l·ªùi:"""
        logger.info(f"[search_web] Prompting Gemini...")
        # logger.debug(f"[search_web] Full Prompt:\n{prompt}") # Uncomment if needed

        response = answering_model.generate_content(prompt)

        # *** LOGGING CHI TI·∫æT PH·∫¢N H·ªíI GEMINI ***
        logger.info(f"[search_web] Raw Gemini Response Text: '{response.text}'")
        try:
            logger.info(f"[search_web] Gemini Prompt Feedback: {response.prompt_feedback}")
            # logger.info(f"[search_web] Gemini Response Parts: {response.parts}") # Uncomment if needed
        except Exception as log_e:
            logger.warning(f"[search_web] Cannot log detailed response info: {log_e}")
        # ******************************************

        final_result = response.text.strip()
        if not final_result:
             logger.warning("[search_web] Gemini returned empty or whitespace response.")
             return {"result": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ th√¥ng tin t√¨m ki·∫øm ƒë∆∞·ª£c tr√™n web."}

        return {"result": final_result}

    except Exception as e:
        logger.error(f"L·ªói trong search_web: {e}")
        return {"result": "ƒê√£ x·∫£y ra l·ªói khi t√¨m ki·∫øm tr√™n web ho·∫∑c x·ª≠ l√Ω k·∫øt qu·∫£."}


# Node ƒë·ªãnh tuy·∫øn (ch·ªçn source)
def route(state: QAState) -> Literal["ministry", "search", "__error__"]:
    logger.info(">>> Running Router Node...")
    query = state.get("query")
    if not query:
        logger.error("Router: No query found in state.")
        # Tr·∫£ v·ªÅ m·ªôt tr·∫°ng th√°i l·ªói ho·∫∑c m·∫∑c ƒë·ªãnh
        return "__error__" # Ho·∫∑c "search" n·∫øu mu·ªën m·∫∑c ƒë·ªãnh

    # Prompt r√µ r√†ng h∆°n, y√™u c·∫ßu output c·ª• th·ªÉ
    routing_prompt = f"""C√¢u h·ªèi sau ƒë√¢y li√™n quan ƒë·∫øn lƒ©nh v·ª±c n√†o?
    1. Th√¥ng tin chung c·∫ßn t√¨m tr√™n web ('search')
    2. Th√¥ng tin chuy√™n s√¢u v·ªÅ B·ªô T√†i ch√≠nh Vi·ªát Nam, vƒÉn b·∫£n, quy ƒë·ªãnh ('ministry')

    C√¢u h·ªèi: "{query}"

    Tr·∫£ l·ªùi CH√çNH X√ÅC b·∫±ng m·ªôt trong hai t·ª´: 'search' ho·∫∑c 'ministry'.
    """
    logger.info("Router: Asking routing_model...")
    try:
        response = routing_model.generate_content(routing_prompt)
        choice = response.text.strip().lower()
        logger.info(f"Router: LLM raw response: '{choice}'")

        # Ki·ªÉm tra ch·∫∑t ch·∫Ω h∆°n
        if "ministry" in choice:
            logger.info("Router: Decision -> ministry")
            return "ministry"
        elif "search" in choice:
            logger.info("Router: Decision -> search")
            return "search"
        else:
            logger.warning(f"Router: Could not determine route from response '{choice}'. Defaulting to 'search'.")
            return "search" # M·∫∑c ƒë·ªãnh t√¨m ki·∫øm web n·∫øu kh√¥ng r√µ

    except Exception as e:
        logger.error(f"Router: Error during routing API call: {e}")
        logger.warning("Router: Error occurred. Defaulting to 'search'.")
        return "search" # M·∫∑c ƒë·ªãnh t√¨m ki·∫øm web khi c√≥ l·ªói

# LangGraph Graph setup
workflow = StateGraph(QAState)

# S·ª≠a l·∫°i lambda ƒë·ªÉ c·∫≠p nh·∫≠t state ƒë√∫ng c√°ch
workflow.add_node("ministry", lambda s: {**s, **search_ministry(s)})
workflow.add_node("search", lambda s: {**s, **search_web(s)})

# Th√™m m·ªôt node x·ª≠ l√Ω l·ªói routing (t√πy ch·ªçn)
workflow.add_node("handle_error", lambda s: {**s, "result": "Xin l·ªói, t√¥i kh√¥ng th·ªÉ ph√¢n lo·∫°i c√¢u h·ªèi c·ªßa b·∫°n."})


# Entry point l√† node ƒë·ªãnh tuy·∫øn
# S·ª≠ d·ª•ng h√†m route ƒë√£ ƒë·ªãnh nghƒ©a ·ªü tr√™n
workflow.set_conditional_entry_point(
    route,
    {
        "ministry": "ministry",
        "search": "search",
        "__error__": "handle_error", # N·∫øu route tr·∫£ v·ªÅ l·ªói    
    }
)

# C√°c node ƒë·ªÅu k·∫øt th√∫c workflow sau khi ch·∫°y
workflow.add_edge("ministry", "__end__")
workflow.add_edge("search", "__end__")
workflow.add_edge("handle_error", "__end__") # Node l·ªói c≈©ng k·∫øt th√∫c

# Bi√™n d·ªãch graph
try:
    app = workflow.compile()
    logger.info("LangGraph compiled successfully.")
except Exception as e:
     logger.error(f"Error compiling LangGraph: {e}")
     raise

# ---------------------------
# Run Q&A bot
# ---------------------------
if __name__ == "__main__":
    logger.info("Starting Q&A Bot...")
    while True:
        try:
            question = input("‚ùì H√£y nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n (ho·∫∑c g√µ 'quit' ƒë·ªÉ tho√°t): ")
            if question.lower() == 'quit':
                break
            if not question.strip():
                print("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")
                continue

            initial_state: QAState = {"query": question, "result": None} # Kh·ªüi t·∫°o state
            logger.info(f"Invoking graph with query: '{question}'")

            # Ch·∫°y graph
            final_state = app.invoke(initial_state)
            logger.info(f"Graph execution finished. Final state: {final_state}")

            # In k·∫øt qu·∫£
            print("-" * 20)
            answer = final_state.get('result', 'Kh√¥ng c√≥ c√¢u tr·∫£ l·ªùi cu·ªëi c√πng.') # L·∫•y k·∫øt qu·∫£ t·ª´ state
            print(f"üìç Tr·∫£ l·ªùi: {answer}")
            print("-" * 20 + "\n")

        except Exception as e:
            # Ghi log l·ªói chi ti·∫øt h∆°n
            logger.exception("An error occurred during the main execution loop:")
            print(f"\nüí• ƒê√£ c√≥ l·ªói x·∫£y ra: {e}")

    logger.info("Q&A Bot stopped.")